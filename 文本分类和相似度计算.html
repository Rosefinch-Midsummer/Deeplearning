<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>文本分类和相似度计算 - Deep Learning</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="前言.html"><strong aria-hidden="true">1.</strong> 前言</a></li><li class="chapter-item expanded affix "><li class="part-title">NN</li><li class="chapter-item expanded affix "><li class="part-title">NLP</li><li class="chapter-item expanded "><a href="NLP.html"><strong aria-hidden="true">2.</strong> NLP</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="全文检索与关键词提取.html"><strong aria-hidden="true">2.1.</strong> 全文检索与关键词提取</a></li><li class="chapter-item expanded "><a href="文本表示和语言模型.html"><strong aria-hidden="true">2.2.</strong> 文本表示和语言模型</a></li><li class="chapter-item expanded "><a href="文本分类和相似度计算.html" class="active"><strong aria-hidden="true">2.3.</strong> 文本分类和相似度计算</a></li><li class="chapter-item expanded "><a href="文本信息抽取.html"><strong aria-hidden="true">2.4.</strong> 文本信息抽取</a></li><li class="chapter-item expanded "><a href="文本生成.html"><strong aria-hidden="true">2.5.</strong> 文本生成</a></li><li class="chapter-item expanded "><a href="智能问答和对话系统.html"><strong aria-hidden="true">2.6.</strong> 智能问答和对话系统</a></li><li class="chapter-item expanded "><a href="大语言模型.html"><strong aria-hidden="true">2.7.</strong> 大语言模型</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">CV</li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Deep Learning</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="文本分类和相似度计算"><a class="header" href="#文本分类和相似度计算">文本分类和相似度计算</a></h1>
<ul>
<li><a href="#%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%BA%A6%E9%87%8F%E4%BB%BB%E5%8A%A1">文本相似度（度量任务）</a>
<ul>
<li><a href="#%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95">文本相似度计算方法</a>
<ul>
<li><a href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95tf-idfcosine_similarity">无监督计算方法：TF-IDF+cosine_similarity</a></li>
<li><a href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95bm25">无监督计算方法——BM25</a></li>
<li><a href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95bert-whitening">无监督计算方法——BERT Whitening</a></li>
<li><a href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E5%8D%95%E5%A1%94%E6%A8%A1%E5%9E%8B%E6%85%A2%E5%87%86%E7%A1%AE%E7%8E%87%E9%AB%98">有监督计算方法——单塔模型（慢，准确率高）</a></li>
<li><a href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E5%BF%AB%E5%87%86%E7%A1%AE%E7%8E%87%E4%BD%8E">有监督计算方法——双塔模型（快，准确率低）</a></li>
<li><a href="#word2vec%E8%AE%A1%E7%AE%97%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6">Word2Vec计算文本相似度</a></li>
<li><a href="#glove%E8%AE%A1%E7%AE%97%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6">GloVe计算文本相似度</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1">文本分类（分类任务）</a>
<ul>
<li><a href="#%E5%9F%BA%E4%BA%8E%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB">基于朴素贝叶斯的文本分类</a>
<ul>
<li><a href="#%E5%AE%9E%E6%88%98%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB">实战：朴素贝叶斯新闻文本分类</a></li>
</ul>
</li>
<li><a href="#bert-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB">BERT 文本分类</a>
<ul>
<li><a href="#%E5%AE%9E%E4%BE%8Bbert%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB">实例：BERT新闻文本分类</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="文本相似度度量任务"><a class="header" href="#文本相似度度量任务">文本相似度（度量任务）</a></h1>
<p>定义：衡量两个文本之间的相似程度。通常用数值度量，数值越高则相似度越高</p>
<h2 id="文本相似度计算方法"><a class="header" href="#文本相似度计算方法">文本相似度计算方法</a></h2>
<p>首先要使用TF-IDF或词向量获取文本特征表示，然后才能计算相似度</p>
<p>方式：</p>
<ul>
<li>无监督相似度计算：度量计算，欧氏距离、余弦距离（1-夹角余弦值）、Jacard相似度（两个集合相似性）、BM25</li>
<li>有监督相似度计算：MLP</li>
</ul>
<h3 id="无监督计算方法tf-idfcosine_similarity"><a class="header" href="#无监督计算方法tf-idfcosine_similarity">无监督计算方法：TF-IDF+cosine_similarity</a></h3>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

documents = [
    "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。",
    "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。",
    "图灵机是计算机科学的一个重要概念，与恩尼格玛机关系密切。",
    "二战时期的科技发展与恩尼格玛机的破解密不可分。",
    "加密技术在信息安全中起着至关重要的作用，恩尼格玛机的历史是一个经典案例。"
]

# 1. 计算 TF-IDF 特征
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# 2. 计算余弦相似度
similarity_matrix = cosine_similarity(tfidf_matrix)

# 3. 输出相似度矩阵
print("文档之间的相似度矩阵：")
print(similarity_matrix)
</code></pre>
<h3 id="无监督计算方法bm25"><a class="header" href="#无监督计算方法bm25">无监督计算方法——BM25</a></h3>
<p>优点：相比于TF-IDF，BM25考虑了文档长度。通过参数调整，BM25可以适应不同领域</p>
<p>缺点：只有统计信息，没有语义信息</p>
<p>gensim包中的BM25算法已经无法正常使用，这里需要执行<code>pip install rank-bm25</code></p>
<pre><code class="language-python">from rank_bm25 import BM25Okapi

documents = [
    "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。",
    "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。",
    "图灵机是计算机科学的一个重要概念，与恩尼格玛机关系密切。",
    "二战时期的科技发展与恩尼格玛机的破解密不可分。",
    "加密技术在信息安全中起着至关重要的作用，恩尼格玛机的历史是一个经典案例。"
]

# 1. 分词
tokenized_documents = [doc.split() for doc in documents]

# 2. 计算 BM25 值
bm25 = BM25Okapi(tokenized_documents)

# 3. 计算相似度
similarity_matrix = []
for i in range(len(tokenized_documents)):
    scores = bm25.get_scores(tokenized_documents[i])
    similarity_matrix.append(scores)

# 4. 输出相似度矩阵
print("文档之间的相似度矩阵：")
for i, scores in enumerate(similarity_matrix):
    print(f"文档 {i}: {scores}")
</code></pre>
<h3 id="无监督计算方法bert-whitening"><a class="header" href="#无监督计算方法bert-whitening">无监督计算方法——BERT Whitening</a></h3>
<p>Bert编码输出得到的句子级别文本向量（【cls】标签），可以直接用来计算文本相似度，但效果比较差</p>
<p>白化Whitening：把Bert的输出向量转换为标准正态分布</p>
<p>两份文本之间的相似度计算：</p>
<pre><code class="language-python">from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import cosine_similarity

# 两个文档
documents = [
    "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。",
    "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。",
]

# 加载预训练的Bert模型和tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese')

# 分词和编码
inputs = tokenizer(documents, return_tensors='pt', padding=True, truncation=True, max_length=128)

# 获取Bert模型的输出
with torch.no_grad():
    outputs = model(**inputs)

# 取出文本的embedding表示
embeddings = outputs.last_hidden_state

# 计算两个文档之间的相似度
similarity = cosine_similarity(embeddings[0].mean(dim=0).reshape(1, -1), embeddings[1].mean(dim=0).reshape(1, -1))

# 输出相似度
print("两个文档之间的相似度为：", similarity)
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>两个文档之间的相似度为： [[0.9393373]]
</code></pre>
<p>计算多份文本的相似度矩阵需要执行<code>pip install sentence_transformers</code></p>
<pre><code class="language-python">from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# 下载Bert模型
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

# 5段短文本
text1 = "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。"
text2 = "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。"
text3 = "图灵机是计算机科学的一个重要概念，与恩尼格玛机关系密切。"
text4 = "二战时期的科技发展与恩尼格玛机的破解密不可分。"
text5 = "加密技术在信息安全中起着至关重要的作用，恩尼格玛机的历史是一个经典案例。"

# 使用Bert模型计算文本之间的相似度
# get the embeddings
embeddings = model.encode([text1, text2, text3, text4, text5])

# calculate the cosine similarity between the embeddings
similarity_matrix = cosine_similarity(embeddings)
print(similarity_matrix)
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>[[1.0000005  0.9500179  0.9116019  0.90483916 0.93870497]
 [0.9500179  1.0000001  0.9029109  0.8839184  0.9437463 ]
 [0.9116019  0.9029109  1.0000002  0.9155248  0.93057024]
 [0.90483916 0.8839184  0.9155248  1.         0.93250304]
 [0.93870497 0.9437463  0.93057024 0.93250304 1.0000001 ]]
</code></pre>
<h3 id="有监督计算方法单塔模型慢准确率高"><a class="header" href="#有监督计算方法单塔模型慢准确率高">有监督计算方法——单塔模型（慢，准确率高）</a></h3>
<p>只进行一次模型计算</p>
<p>使用过程：</p>
<ol>
<li>将待匹配的两个文本使用Bert中【SEP】特殊符号拼接</li>
<li>输入进Bert编码，学习两个文本之间的语义关系</li>
<li>输出【CLS】向量表示</li>
<li>添加全连接层进行二分类，0表示不相似，1表示相似</li>
</ol>
<p>优点：可以学到句子之间的深层语义关系，准确率高</p>
<p>缺点：拼接后文本长度可能过长，编码速度慢。需要两两拼接文本，若文本数量过多，该过程非常耗时</p>
<h3 id="有监督计算方法双塔模型快准确率低"><a class="header" href="#有监督计算方法双塔模型快准确率低">有监督计算方法——双塔模型（快，准确率低）</a></h3>
<p>两次模型计算，即两个文本分布计算一次</p>
<p>使用过程：</p>
<ol>
<li>将两个文本单独输入Bert编码，获得各自的【CLS】向量表示</li>
<li>通过度量方法（余弦距离、MLP层）等计算相似度</li>
</ol>
<p>优点：每个文本都只需编码一次，计算相似度时可以直接用。不必多次编码，效率高</p>
<p>缺点：两个文本之间缺乏深层的语义交互</p>
<p>两大模型：</p>
<ol>
<li>DSSM：Bert编码+余弦相似度计算文本相似性</li>
<li>Sentence Transformer：Bert编码+Softmax层计算文本相似性</li>
</ol>
<h3 id="word2vec计算文本相似度"><a class="header" href="#word2vec计算文本相似度">Word2Vec计算文本相似度</a></h3>
<pre><code class="language-python">from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 导入文本
text1 = file_contents
text2 = text_content

# 使用gensim内置的Word2Vec工具训练Word2Vec模型
sentences = [text1.split(), text2.split()]  # 将文本切分为单词列表
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # 训练Word2Vec模型

# 计算均值词向量
def get_vector_mean(text, model):
    words = text.split()
    vectors = [model.wv[word] for word in words if word in model.wv]
    if len(vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(vectors, axis=0)

mean_vector1_w2v = get_vector_mean(text1, model)
mean_vector2_w2v = get_vector_mean(text2, model)

# 计算相似度
similarity_w2v = cosine_similarity([mean_vector1_w2v], [mean_vector2_w2v])[0][0]

# 输出结果
print(f"Word2Vec Similarity: {similarity_w2v:.4f}")
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>Word2Vec Similarity: 0.8934
</code></pre>
<h3 id="glove计算文本相似度"><a class="header" href="#glove计算文本相似度">GloVe计算文本相似度</a></h3>
<p>引例</p>
<pre><code class="language-python">import gensim.downloader as api
model = api.load('glove-wiki-gigaword-100')

print(model["bread"])
print(model.most_similar("usa"))
print(model.most_similar(negative="banana"))
</code></pre>
<p>计算文本相似度</p>
<pre><code class="language-python">import numpy as np
from gensim.models import KeyedVectors

# 加载 GloVe 词向量
def load_glove_model(glove_model_path, encoding='utf-8'):
    # 打开GloVe模型文件并读取内容
    with open(glove_model_path, 'r', encoding=encoding) as file:
        model = {}
        for line in file:
            split_line = line.strip().split(' ')
            word = split_line[0]
            embedding = [float(val) for val in split_line[1:]]
            model[word] = embedding
    return model

# 然后在main()函数中使用更新后的load_glove_model函数
#glove_model = load_glove_model(glove_model_path, encoding='utf-8')
def load_glove_model2(glove_file):
    """
    加载 GloVe 词向量
    :param glove_file: GloVe 文件路径
    :return: 词向量字典
    """
    glove_model = {}
    with open(glove_file, 'r', encoding='utf-8') as f:
        for line in f:
            split_line = line.split()
            word = split_line[0]
            vector = np.array([float(val) for val in split_line[1:]])
            glove_model[word] = vector
    return glove_model

# 计算文本的词向量均值
def get_vector_mean(text, model):
    """
    计算文本的词向量均值
    :param text: 输入文本
    :param model: 词向量模型
    :return: 均值词向量
    """
    words = text.split()  # 将文本分词
    word_vectors = []

    for word in words:
        if word in model:
            word_vectors.append(model[word])

    if not word_vectors:
        return np.zeros(model.vector_size)  # 如果没有有效词向量，则返回零向量

    return np.mean(word_vectors, axis=0)  # 计算均值

# 计算余弦相似度
def cosine_similarity(vec1, vec2):
    """
    计算两个向量的余弦相似度
    :param vec1: 第一个向量
    :param vec2: 第二个向量
    :return: 余弦相似度
    """
    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
        return 0.0  # 避免除以零的情况
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# 主函数示例
def main():
    # 加载模型
    glove_model_path = '/content/glove.6B.100d.txt'

    glove_model = load_glove_model(glove_model_path, encoding="utf-8")

    # 导入文本
    text1 = file_contents
    text2 = text_content

    # 计算均值词向量

    mean_vector1_glove = get_vector_mean(text1, glove_model)
    mean_vector2_glove = get_vector_mean(text2, glove_model)

    # 计算相似度
    similarity_glove = cosine_similarity(mean_vector1_glove, mean_vector2_glove)

    # 输出结果
    print(f"GloVe Similarity: {similarity_glove:.4f}")

if __name__ == "__main__":
    main()
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>GloVe Similarity: 0.9933
</code></pre>
<h1 id="文本分类分类任务"><a class="header" href="#文本分类分类任务">文本分类（分类任务）</a></h1>
<p>什么是文本分类？</p>
<p>文本分类是指计算机将一篇载有信息的文本映射到预先给定的某一类别或某几类别主题的过程。</p>
<p>主要应用：新闻主题分类、情感分析、舆情分析、邮件过滤等</p>
<h2 id="基于朴素贝叶斯的文本分类"><a class="header" href="#基于朴素贝叶斯的文本分类">基于朴素贝叶斯的文本分类</a></h2>
<p>基本思想：基于贝叶斯定理确定文本属于某一类别的概率，选择具有最高概率的类别作为文本分类结果</p>
<p>特征独立性假设：认为文本中的特征（词语或单词）在给定类别下相互独立</p>
<p>基本流程：</p>
<ol>
<li>将文本通过词袋模型或TF-IDF表示为特征向量</li>
<li>通过文本的特征向量与标签，朴素贝叶斯分类器学习类别和单词之间的条件概率分布，即计算给定类别下，每个单词出现的条件概率</li>
<li>给定一个新文本，分类器计算它属于每个类别的条件概率，选择具有最高条件概率的类别作为最终的分类结果</li>
</ol>
<h3 id="实战朴素贝叶斯新闻文本分类"><a class="header" href="#实战朴素贝叶斯新闻文本分类">实战：朴素贝叶斯新闻文本分类</a></h3>
<h2 id="bert-文本分类"><a class="header" href="#bert-文本分类">BERT 文本分类</a></h2>
<p>基本流程：</p>
<ol>
<li>将【CLS】位置对应的输出作为句子表示</li>
<li>送入全连接层（Dense），将句子表示为hidden_dim维映射到label_dim维（类别总数）</li>
<li>经过Softmax函数处理，获得该句子属于各类别的概率</li>
<li>概率最大值对应的类别为句子的预测标签</li>
</ol>
<h3 id="实例bert新闻文本分类"><a class="header" href="#实例bert新闻文本分类">实例：BERT新闻文本分类</a></h3>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="文本表示和语言模型.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="文本信息抽取.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="文本表示和语言模型.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="文本信息抽取.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
