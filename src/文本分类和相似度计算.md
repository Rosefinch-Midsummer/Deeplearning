# 文本分类和相似度计算

<!-- toc -->

# 文本相似度（度量任务）

定义：衡量两个文本之间的相似程度。通常用数值度量，数值越高则相似度越高

## 文本相似度计算方法

首先要使用TF-IDF或词向量获取文本特征表示，然后才能计算相似度

方式：

- 无监督相似度计算：度量计算，欧氏距离、余弦距离（1-夹角余弦值）、Jacard相似度（两个集合相似性）、BM25
- 有监督相似度计算：MLP

### 无监督计算方法：TF-IDF+cosine_similarity


```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

documents = [
    "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。",
    "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。",
    "图灵机是计算机科学的一个重要概念，与恩尼格玛机关系密切。",
    "二战时期的科技发展与恩尼格玛机的破解密不可分。",
    "加密技术在信息安全中起着至关重要的作用，恩尼格玛机的历史是一个经典案例。"
]

# 1. 计算 TF-IDF 特征
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# 2. 计算余弦相似度
similarity_matrix = cosine_similarity(tfidf_matrix)

# 3. 输出相似度矩阵
print("文档之间的相似度矩阵：")
print(similarity_matrix)
```

### 无监督计算方法——BM25

优点：相比于TF-IDF，BM25考虑了文档长度。通过参数调整，BM25可以适应不同领域

缺点：只有统计信息，没有语义信息

gensim包中的BM25算法已经无法正常使用，这里需要执行`pip install rank-bm25`

```python
from rank_bm25 import BM25Okapi

documents = [
    "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。",
    "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。",
    "图灵机是计算机科学的一个重要概念，与恩尼格玛机关系密切。",
    "二战时期的科技发展与恩尼格玛机的破解密不可分。",
    "加密技术在信息安全中起着至关重要的作用，恩尼格玛机的历史是一个经典案例。"
]

# 1. 分词
tokenized_documents = [doc.split() for doc in documents]

# 2. 计算 BM25 值
bm25 = BM25Okapi(tokenized_documents)

# 3. 计算相似度
similarity_matrix = []
for i in range(len(tokenized_documents)):
    scores = bm25.get_scores(tokenized_documents[i])
    similarity_matrix.append(scores)

# 4. 输出相似度矩阵
print("文档之间的相似度矩阵：")
for i, scores in enumerate(similarity_matrix):
    print(f"文档 {i}: {scores}")
```

### 无监督计算方法——BERT Whitening

Bert编码输出得到的句子级别文本向量（【cls】标签），可以直接用来计算文本相似度，但效果比较差

白化Whitening：把Bert的输出向量转换为标准正态分布

两份文本之间的相似度计算：

```python
from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import cosine_similarity

# 两个文档
documents = [
    "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。",
    "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。",
]

# 加载预训练的Bert模型和tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese')

# 分词和编码
inputs = tokenizer(documents, return_tensors='pt', padding=True, truncation=True, max_length=128)

# 获取Bert模型的输出
with torch.no_grad():
    outputs = model(**inputs)

# 取出文本的embedding表示
embeddings = outputs.last_hidden_state

# 计算两个文档之间的相似度
similarity = cosine_similarity(embeddings[0].mean(dim=0).reshape(1, -1), embeddings[1].mean(dim=0).reshape(1, -1))

# 输出相似度
print("两个文档之间的相似度为：", similarity)
```

输出结果如下所示：

```
两个文档之间的相似度为： [[0.9393373]]
```

计算多份文本的相似度矩阵需要执行`pip install sentence_transformers`

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# 下载Bert模型
model = SentenceTransformer('distilbert-base-nli-mean-tokens')

# 5段短文本
text1 = "恩尼格玛机是二战时期纳粹德国使用的加密机器，后被英国破译，参与破译的人员有被称为计算机科学之父、人工智能之父的图灵。"
text2 = "恩尼格玛机使用的加密方式本质上还是移位和替代，只不过因为密码表种类极多，破解难度高，同时加密解密机器化，使用便捷，因而在二战时期得以使用。"
text3 = "图灵机是计算机科学的一个重要概念，与恩尼格玛机关系密切。"
text4 = "二战时期的科技发展与恩尼格玛机的破解密不可分。"
text5 = "加密技术在信息安全中起着至关重要的作用，恩尼格玛机的历史是一个经典案例。"

# 使用Bert模型计算文本之间的相似度
# get the embeddings
embeddings = model.encode([text1, text2, text3, text4, text5])

# calculate the cosine similarity between the embeddings
similarity_matrix = cosine_similarity(embeddings)
print(similarity_matrix)
```

输出结果如下所示：

```
[[1.0000005  0.9500179  0.9116019  0.90483916 0.93870497]
 [0.9500179  1.0000001  0.9029109  0.8839184  0.9437463 ]
 [0.9116019  0.9029109  1.0000002  0.9155248  0.93057024]
 [0.90483916 0.8839184  0.9155248  1.         0.93250304]
 [0.93870497 0.9437463  0.93057024 0.93250304 1.0000001 ]]
```

### 有监督计算方法——单塔模型（慢，准确率高）

只进行一次模型计算

使用过程：

1. 将待匹配的两个文本使用Bert中【SEP】特殊符号拼接
2. 输入进Bert编码，学习两个文本之间的语义关系
3. 输出【CLS】向量表示
4. 添加全连接层进行二分类，0表示不相似，1表示相似

优点：可以学到句子之间的深层语义关系，准确率高

缺点：拼接后文本长度可能过长，编码速度慢。需要两两拼接文本，若文本数量过多，该过程非常耗时
### 有监督计算方法——双塔模型（快，准确率低）

两次模型计算，即两个文本分布计算一次

使用过程：

1. 将两个文本单独输入Bert编码，获得各自的【CLS】向量表示
2. 通过度量方法（余弦距离、MLP层）等计算相似度

优点：每个文本都只需编码一次，计算相似度时可以直接用。不必多次编码，效率高

缺点：两个文本之间缺乏深层的语义交互

两大模型：

1. DSSM：Bert编码+余弦相似度计算文本相似性
2. Sentence Transformer：Bert编码+Softmax层计算文本相似性

### Word2Vec计算文本相似度

```python
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 导入文本
text1 = file_contents
text2 = text_content

# 使用gensim内置的Word2Vec工具训练Word2Vec模型
sentences = [text1.split(), text2.split()]  # 将文本切分为单词列表
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # 训练Word2Vec模型

# 计算均值词向量
def get_vector_mean(text, model):
    words = text.split()
    vectors = [model.wv[word] for word in words if word in model.wv]
    if len(vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(vectors, axis=0)

mean_vector1_w2v = get_vector_mean(text1, model)
mean_vector2_w2v = get_vector_mean(text2, model)

# 计算相似度
similarity_w2v = cosine_similarity([mean_vector1_w2v], [mean_vector2_w2v])[0][0]

# 输出结果
print(f"Word2Vec Similarity: {similarity_w2v:.4f}")
```

输出结果如下所示：

```
Word2Vec Similarity: 0.8934
```

### GloVe计算文本相似度

引例

```python
import gensim.downloader as api
model = api.load('glove-wiki-gigaword-100')

print(model["bread"])
print(model.most_similar("usa"))
print(model.most_similar(negative="banana"))
```

计算文本相似度

```python
import numpy as np
from gensim.models import KeyedVectors

# 加载 GloVe 词向量
def load_glove_model(glove_model_path, encoding='utf-8'):
    # 打开GloVe模型文件并读取内容
    with open(glove_model_path, 'r', encoding=encoding) as file:
        model = {}
        for line in file:
            split_line = line.strip().split(' ')
            word = split_line[0]
            embedding = [float(val) for val in split_line[1:]]
            model[word] = embedding
    return model

# 然后在main()函数中使用更新后的load_glove_model函数
#glove_model = load_glove_model(glove_model_path, encoding='utf-8')
def load_glove_model2(glove_file):
    """
    加载 GloVe 词向量
    :param glove_file: GloVe 文件路径
    :return: 词向量字典
    """
    glove_model = {}
    with open(glove_file, 'r', encoding='utf-8') as f:
        for line in f:
            split_line = line.split()
            word = split_line[0]
            vector = np.array([float(val) for val in split_line[1:]])
            glove_model[word] = vector
    return glove_model

# 计算文本的词向量均值
def get_vector_mean(text, model):
    """
    计算文本的词向量均值
    :param text: 输入文本
    :param model: 词向量模型
    :return: 均值词向量
    """
    words = text.split()  # 将文本分词
    word_vectors = []

    for word in words:
        if word in model:
            word_vectors.append(model[word])

    if not word_vectors:
        return np.zeros(model.vector_size)  # 如果没有有效词向量，则返回零向量

    return np.mean(word_vectors, axis=0)  # 计算均值

# 计算余弦相似度
def cosine_similarity(vec1, vec2):
    """
    计算两个向量的余弦相似度
    :param vec1: 第一个向量
    :param vec2: 第二个向量
    :return: 余弦相似度
    """
    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
        return 0.0  # 避免除以零的情况
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# 主函数示例
def main():
    # 加载模型
    glove_model_path = '/content/glove.6B.100d.txt'

    glove_model = load_glove_model(glove_model_path, encoding="utf-8")

    # 导入文本
    text1 = file_contents
    text2 = text_content

    # 计算均值词向量

    mean_vector1_glove = get_vector_mean(text1, glove_model)
    mean_vector2_glove = get_vector_mean(text2, glove_model)

    # 计算相似度
    similarity_glove = cosine_similarity(mean_vector1_glove, mean_vector2_glove)

    # 输出结果
    print(f"GloVe Similarity: {similarity_glove:.4f}")

if __name__ == "__main__":
    main()
```

输出结果如下所示：

```
GloVe Similarity: 0.9933
```
# 文本分类（分类任务）

什么是文本分类？

文本分类是指计算机将一篇载有信息的文本映射到预先给定的某一类别或某几类别主题的过程。

主要应用：新闻主题分类、情感分析、舆情分析、邮件过滤等



## 基于朴素贝叶斯的文本分类


基本思想：基于贝叶斯定理确定文本属于某一类别的概率，选择具有最高概率的类别作为文本分类结果

特征独立性假设：认为文本中的特征（词语或单词）在给定类别下相互独立

基本流程：

1. 将文本通过词袋模型或TF-IDF表示为特征向量
2. 通过文本的特征向量与标签，朴素贝叶斯分类器学习类别和单词之间的条件概率分布，即计算给定类别下，每个单词出现的条件概率
3. 给定一个新文本，分类器计算它属于每个类别的条件概率，选择具有最高条件概率的类别作为最终的分类结果

### 实战：朴素贝叶斯新闻文本分类




## BERT 文本分类

基本流程：

1. 将【CLS】位置对应的输出作为句子表示
2. 送入全连接层（Dense），将句子表示为hidden_dim维映射到label_dim维（类别总数）
3. 经过Softmax函数处理，获得该句子属于各类别的概率
4. 概率最大值对应的类别为句子的预测标签


### 实例：BERT新闻文本分类



