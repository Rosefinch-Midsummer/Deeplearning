# 文本表示和语言模型

<!-- toc -->

## 文本表示


### 文本向量化

文本数据：典型的序列数据、字符序列或单词序列

文本向量化：将文本数据转化为数值张量，尽可能保留语法、语义信息

目的：将文本表示为一系列能够表达文本语义的向量，方便后续处理

方法：

- 按词切分，将每个单词转换为一个向量
- 按字符切分，将每个字符转换为一个向量
- 按n-gram切分，将每个n-gram转换为一个向量

n-gram：多个连续单词或字符的集合，注意n-gram之间可重叠

Token：单词、字符、n-gram

中文需要专门的分词工具

文本表示编码方式：独热编码、词向量（词嵌入）编码

### 词袋模型：独热编码

基本思想：将一段文本仅看作一些独立的词语的集合，忽略文本的词序、语法和句法

词袋模型统计在一个句子中每个单词出现的情况，可以是one-hot编码（是否出现）、TF编码（次数）、TF-IDF编码（TF-IDF值）

特点：字典中的字没有特定的顺序，句子的整体结构被抛弃

构建过程：

1. 对文档集中的所有文本进行分词
2. 构建整个文档集的词典，假设词典大小为N
3. 为每条文本生成长度为N的一维向量，向量中的每一维的值为字典中对应序号的词在该文本中出现的次数

优点：方便分类器处理离散数据、在一定程度上起到了扩充特征的作用

缺点：没有考虑词序、忽略了词与词之间的关系，得到的特征表示是离散稀疏的


```python
from sklearn.feature_extraction.text import CountVectorizer
import jieba

corpus = [
    "我是中国人，我爱中国",
    "我是上海人",
    "我住在松江大学城"
]

split_corpus = []

for text in corpus:
  new_text = " ".join(jieba.lcut(text))
  split_corpus.append(new_text)
print(split_corpus)
```

输出结果如下所示：

```
['我 是 中国 人 ， 我 爱 中国', '我 是 上海 人', '我 住 在 松江 大学城']
```

```python
# 生成词袋
cv = CountVectorizer()
cv_fit = cv.fit_transform(split_corpus)
print(cv.get_feature_names_out()) #显示特征列表
print(cv_fit.toarray()) #显示特征向量
```

输出结果（词语）如下所示：

```
['上海' '中国' '大学城' '松江']
[[0 2 0 0]
 [1 0 0 0]
 [0 0 1 1]]
```

```python
# 生成词袋
cv = CountVectorizer(token_pattern=r"(?u)\b\w+\b")
cv_fit = cv.fit_transform(split_corpus)
print(cv.get_feature_names_out()) #显示特征列表
print(cv_fit.toarray()) #显示特征向量
```

输出结果（字词）如下所示：

```
['上海' '中国' '人' '住' '在' '大学城' '我' '是' '松江' '爱']
[[0 2 1 0 0 0 2 1 0 1]
 [1 0 1 0 0 0 1 1 0 0]
 [0 0 0 1 1 1 1 0 1 0]]
```

方法一：先向量化，再使用TF-IDF

```python
from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()
tfidf_fit = tfidf_transformer.fit_transform(cv_fit)
print(tfidf_fit.toarray())
```

方法二：直接使用TfidfVectorizer计算

```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(token_pattern=r"(?u)\b\w+\b")
tfidf_fit = tfidf.fit_transform(split_corpus)
print(tfidf_fit.toarray())
```


### 词嵌入（词向量）表示

定义：通过大量语料的训练，学习单词之间的关系，将每个词都映射到一个较短的词向量

词向量特征：低维、密集、训练得到

构建过程：

1. 对文档集中的所有文本进行分词
2. 构建整个文档集的词典，每个词用一个低维向量表示
3. 为每条文本生成一个二维数组，数组中的每一维的值为字典中对应序号的词的一维向量


## 语言模型

A language model is a probability distribution over sequence of works. ----Dan Jurafsky

给定词典V，能够计算出任意单词序列w1,w2，……，wn是一句话的概率

n-gram模型：第n个词的出现只与前n-1个词相关，与其他词无关


## 静态语言模型

语言模型认为语句中下一个单词的出现与其前面的单词有关，用其前面的单词预测下一个单词

### Word2Vec模型

语句中的单词只与其相近的单词有关，用其前面的单词预测下一个单词

主要分为两种类型：

- CBOW：根据周围词预测中心词，输入：某一个特征值的上下文相关的词对应的词向量，输出：该特征词的词向量
- Skip-gram：根据中心词预测周围词，输入：特定的词向量，输出：特定词对应的上下文词向量

基本思想：从大量文本语料中通过无监督方式学习语义知识的简单神经网络

作用：生成上下文语义相关的词向量

模型结构：输入层、隐藏层、输出层

输入：每个单词的One-hot编码

输出：词汇表中每个单词的概率

优点：

1. 通过大量语料进行无监督训练得到词向量，不需要人工参与
2. 考虑了单词之间的上下文关系
3. 训练得到的词向量可以计算单词之间的相似度、语义关系
4. 训练得到的词向量维度较低，可以避免维度灾难
5. 通用性强

缺点：

1. 词向量的语义受训练语料的影响
2. 训练得到的词向量是静态的，词与向量一对一固定，无法更换，无法解决一词多义的问题
3. 只考虑指定上下文窗口内的信息，没有考虑全局的信息
4. 无法针对特定任务进行动态优化


### GloVe模型

改进Word2Vec，利用语料库的全局信息表示词向量

基本思想：考虑单词与单词之间的共现信息，通过构建词共现矩阵来学习单词的词向量

共现矩阵X，其中Xij表示语料库中单词i和单词j共同出现在一个窗口中的次数

优点：考虑了单词之间的全局信息

缺点：

1. 训练得到的词向量仍然是静态的，词与向量一对一固定，无法更改，无法解决一词多义的问题
2. 无法针对特定任务进行动态优化


## 预训练语言模型

### Transformer模型

每层encoder结构：一个自注意力层+1个前向神经网络层

每层decoder结构：一个自注意力层+1个encoder-decoder注意力层+1个前向神经网络层

### BERT模型

全称：Bidirectional Encoder Representations from Transformers

BERT只采用了Transformer中的Encoder结构

优点：具有更强的语言表征能力和特征提取能力

两个预训练任务：

1. 遮盖语言模型（Masked Language Model），目的：使模型学习到单词在上下文中的分布。做法：随机遮盖掉输入序列中的部分Token，让模型预测被遮盖掉的token。
2. 下一句预测（Next Sentence Prediction），目的：使模型理解两个句子之间的关系，有助于QA、NLI任务。做法：对于A、B两句话，判断B是不是A的下一句。

Bert的输入Embedding由三种embedding（Token Embedding、Segment Embedding、Position Embedding


## 大语言模型

大模型基石——特征表示模型

多头注意力机制、残差链接等深度网络的优化技术

支持文本、语音、图像等多种模态的特征表示

ChatGPT：在语言模型的基础上进行对话优化，基础：指令学习（instruction learning）、人类反馈强化学习（HFRL）
