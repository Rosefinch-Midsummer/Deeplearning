# 全文检索与关键词提取

<!-- toc -->

## 关键词提取

什么是关键词提取？

关键词提取是自动提取反映文本主题或内容的词和短语的过程、技术和方法

常用方法：

- 有监督学习：将关键词提取问题转换为判断每个候选关键词是否为关键词的二分类问题，需要已标注关键词的文档集来训练分类模型
- 无监督学习：利用特定的方法提取文本中比较重要的候选词，通过对各候选词打分，选择分数最高的若干候选词

### 无监督——基于文本统计特征TF-IDF

TF-IDF=TF\*IDF

特点：基于统计，简单迅速，但没有考虑词语的语义信息，无法处理一词多义、一义多词

```python
import jieba
import jieba.posseg

corpus = [
    "本书是反思及批判“公共知识分子”的社会学名著，也是研究当代思想史的重要参考资料。本书从观念、经济、社会、媒体、法律、战争等6个方面全面陈述了知识分子在社会发展中的舆论导向作用。一系列重大问题上的官方政策的形成都会受到知识分子所塑造的舆论氛围的影响。 当代知识分子不仅影响力胜过以前，而且发挥影响力的方式也大有不同。他们并不是通过塑造执政者的观点或引导执政者的行动来影响事件进程，而是通过影响民主社会中的执政者的行动的各种方式，来塑造公共舆论，最终影响事件进程。无论执政者是否接受知识分子的一般构想或者决策，知识分子的这种影响都会实现。《知识分子与社会》通过大量历史和现实的案例，深入、全面分析了知识分子导致社会变动的背景、诱因和巨大后果。",
    "托马斯·索维尔（Thomas Sowell），美国当代杰出的经济学家、最具影响力的社会评论家。先后在康奈尔大学、加州大学洛杉矶分校、哥伦比亚大学以及斯坦福大学讲授经济学课程，现为斯坦福大学胡佛研究所公共政策高级研究员。 托马斯·索维尔在经济学和广泛的人文社科领域均有重要建树。他撰写了包括《基础经济学》、《被掩盖的经济真相》、《美国种族简史》在内的30余部著作，多本被翻译成法文、德文、西班牙文和日文出版，其中多本是亚马逊网上书店的超级畅销书。他还在《福布斯》、《财富》、《新闻周刊》、《时代》周刊、《华尔街日报》、《华盛顿邮报》等主流媒体上发表了大量文章，并担任多家著名媒体的专栏作家，广泛讨论各种社会问题。 他获得的荣誉包括由美国企业研究所颁发的、备受尊敬的“博伊尔奖”（Francis Boyer Award）、美国“国家人文科学奖章”、布莱德雷基金奖等。",
    "大清国什么时候灭亡啊？"
]

stopwords = ["的"]

def Process():
  filter_document = []
  for text in corpus:
    segment = jieba.posseg.cut(text.strip()) #分词并进行词性标注
    filter_words = []
    for word, flag in segment: #单词，词性
      if flag.startswith('n') is False: #不是名词
        continue
      if not word in stopwords and len(word) > 1:
        filter_words.append(word)
      filter_document.append(filter_words)
  return filter_document
filter_document = Process()
print(filter_document)
```

```python
#计算TF值
tf_dict = {}
for word in text:
  if word not in tf_dict:
    tf_dict[word] = 1
  else:
    tf_dict[word] += 1
print(tf_dict)
for word in tf_dict:
  tf_dict[word] = tf_dict[word] / len(text)
print(tf_dict)
```

```python
#依次统计每个单词的IDF值
import math
idf_dict = {}
for text in filter_document:
  for word in set(text):
    if word not in idf_dict.keys():
      idf_dict[word] = 1
    else:
      idf_dict[word] += 1
#计算IDF
for word in idf_dict.keys():
  idf_dict[word] = math.log(len(filter_document) / (idf_dict[word] + 1))
print(idf_dict)
```

```python
#计算TF-IDF
tf_idf_dict = {}
for word in text:
  if word not in idf_dict:
    idf_dict[word] = 0
  tf_idf_dict[word] = tf_dict[word] * idf_dict[word]

```

```python
# 降序排序
sorted_tf_idf = sorted(tf_idf_dict.items(), key=lambda x:x[1], reverse=True)
print(sorted_tf_idf)
```

```python
print('\n第{}个文本的关键词是:{}\n'.format(1, sorted_tf_idf[0][0]))
```


### 无监督——基于词图TextRank

TextRank算法由网页重要性排序的PageRank算法改进而来

TextRank算法是基于图的用于文本摘要和关键词提取的算法，利用文档内部词语间的共现信息进行抽取。

用图模型表示文本，节点是文本中的单词或短语，边是单词之间的关系，通过迭代更新节点的权重来计算每个结点的重要程度。

```python
import jieba
import jieba.posseg

corpus = [
    "本书是反思及批判“公共知识分子”的社会学名著，也是研究当代思想史的重要参考资料。本书从观念、经济、社会、媒体、法律、战争等6个方面全面陈述了知识分子在社会发展中的舆论导向作用。一系列重大问题上的官方政策的形成都会受到知识分子所塑造的舆论氛围的影响。 当代知识分子不仅影响力胜过以前，而且发挥影响力的方式也大有不同。他们并不是通过塑造执政者的观点或引导执政者的行动来影响事件进程，而是通过影响民主社会中的执政者的行动的各种方式，来塑造公共舆论，最终影响事件进程。无论执政者是否接受知识分子的一般构想或者决策，知识分子的这种影响都会实现。《知识分子与社会》通过大量历史和现实的案例，深入、全面分析了知识分子导致社会变动的背景、诱因和巨大后果。",
    "托马斯·索维尔（Thomas Sowell），美国当代杰出的经济学家、最具影响力的社会评论家。先后在康奈尔大学、加州大学洛杉矶分校、哥伦比亚大学以及斯坦福大学讲授经济学课程，现为斯坦福大学胡佛研究所公共政策高级研究员。 托马斯·索维尔在经济学和广泛的人文社科领域均有重要建树。他撰写了包括《基础经济学》、《被掩盖的经济真相》、《美国种族简史》在内的30余部著作，多本被翻译成法文、德文、西班牙文和日文出版，其中多本是亚马逊网上书店的超级畅销书。他还在《福布斯》、《财富》、《新闻周刊》、《时代》周刊、《华尔街日报》、《华盛顿邮报》等主流媒体上发表了大量文章，并担任多家著名媒体的专栏作家，广泛讨论各种社会问题。 他获得的荣誉包括由美国企业研究所颁发的、备受尊敬的“博伊尔奖”（Francis Boyer Award）、美国“国家人文科学奖章”、布莱德雷基金奖等。",
    "美国总统特朗普不是一个大坏蛋"
]
stopwords = ["的"]

def Process():
  filter_document = []
  for text in corpus:
    segment = jieba.posseg.cut(text.strip()) #分词并进行词性标注
    filter_words = []
    for word, flag in segment: #单词，词性
      if flag.startswith('n') is False:
        continue
      if not word in stopwords and len(word) > 1:
        filter_words.append(word)
    filter_document.append(filter_words)
  return filter_document
filter_document = Process()
print(filter_document)
```

```python
from collections import defaultdict

# 构建共现矩阵
def build_cooccurrence_matrix(sentences):
  cooccur_matrix = defaultdict(int)
  for sentence in sentences:
    #对每个文本，统计两两单词的共现次数
    for i, word1 in enumerate(sentence):
      for j, word2 in enumerate(sentence):
        if i != j:
          cooccur_matrix[(word1, word2)] += 1
  return cooccur_matrix

```

```python
#迭代更新TextRank分数
def calculate_textrank_scores(cooccur_matrix, max_iter=100, d=0.85):
  #max_iter:迭代次数
  #d:阻尼系数
  word_scores = defaultdict(float)
  word_weights = defaultdict(float)

  for (word1, word2), cooccur_count in cooccur_matrix.items():
    word_weights[word1] += cooccur_count
    word_weights[word2] += cooccur_count

  #初始化每个单词的权重
  for word in word_weights:
    word_scores[word] = 1.0

  #开始迭代
  for _ in range(max_iter):
    new_word_scores = defaultdict(float)
    for word1, word2 in cooccur_matrix:
      if word1 == word2:
        continue
      new_word_scores[word2] += cooccur_matrix[(word1, word2)] / word_weights[word1] * word_scores[word1]
    #更新每个单词的权重
    for word in word_scores:
      new_word_scores[word] = (1-d) + d*new_word_scores[word]
    word_scores = new_word_scores
  return word_scores
```

```python
#对每个文本得到更新后的每个单词的权重，输出Top3的关键词
for index, text in enumerate(filter_document):
  cooccur_matrix = build_cooccurrence_matrix([text])
  word_scores = calculate_textrank_scores(cooccur_matrix)
  sorted_scores = sorted(word_scores.items(), key=lambda x:x[1], reverse=True)
  keywords = [word for word, score in sorted_scores]
  print('\n第{}个文本的关键词是:{}\n'.format(index, keywords[index]))

```

输出结果如下所示：

```
第0个文本的关键词是:知识分子

第1个文本的关键词是:经济学

第2个文本的关键词是:大坏蛋
```

### 无监督——基于主题模型LSA/LDA

基本思想：利用主题模型中的主题分布性质进行关键词提取

假设文档生成过程：以一定概率选取某个主题，再以一定的概率选取该主题下的某个单词，不断重复这两个步骤，生成文档


```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# 自定义中文停用词列表
#stopwords = ['的', '了', '是', '我', '你', '他']  # 添加你认为需要过滤的停用词

# 对中文文本进行分词处理
#file_contents_cut = [" ".join([word for word in jieba.lcut(doc) if word not in stopwords]) for doc in file_contents]
file_contents_cut = file_contents
#print(file_contents_cut)
# 将文档集字符串转换为向量表示，添加min_df参数来解决空词汇表问题
vectorizer = CountVectorizer(min_df=2)
X = vectorizer.fit_transform(file_contents_cut.split('\n'))

# 使用LDA算法进行主题分析
lda = LatentDirichletAllocation(n_components=5, random_state=0)  # 假设有5个主题
lda.fit(X)

# 获取主题-词分布
topic_word_distributions = lda.components_

# 输出每个主题的前10个主题词
n_top_words = 10
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(topic_word_distributions):
    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
    top_features = [feature_names[i] for i in top_features_ind]
    print(f"Topic {topic_idx}: {' '.join(top_features)}")
```

## 基于关键词的检索

### 全文检索

倒排索引Inverted Index：words to document

基本原理：通过建立倒排索引，将文本数据中的单词映射到包含这些单词的文档或记录，从而实现高效的文本检索。

应用场景：数据量大、数据结构不固定的数据采用全文检索搜索，如搜索引擎、电商网站搜索等。

全文检索具体步骤：

1. 文本分词、去除停用词
2. 创建单词词典
3. 为每个单词创建一个倒排列表，包含该单词所在的文档及出现的位置（离线过程）
4. 当用户提交查询时，将查询词与倒排索引匹配，找到包含查询词的文档（在线过程）
5. 根据相关性对文档进行排序，并返回给用户


### 全文检索工具：Lucene

应用场景：直接通过代码调用接口，实现文本索引和检索；适用于数据索引量不大

使用：

1. 创建索引：分词、建立词典表和倒排索引，写入索引库
2. 查询索引：对查询语句进行词法语法分析，搜索索引，对搜索结果进行排序

### 全文检索工具：ElasticSearch

ElasticSearch是Lucene的封装，提供了REST API的访问接口

核心是一个分布式文档存储、检索和分析系统

特点：

- 分布式性能
- 实时搜索
- 多样化查询功能：全文搜索、聚合、过滤、排序、分词和模糊搜索
- 强大的数据分析

开发视角：

- 文档（Document）：可搜索的最小单元
- 字段（Field）：文档里的每条信息，类似于数据库中的列
- 索引（Index）：相同类型的文档集合



### 领域应用：未登录词

未登录词：已有的词表中没有收录的词或已有的训练语料中未曾出现过的词

识别未登录词

如何断定n-gram的短语是个词语？

短语左右搭配丰富（信息熵），短语内部成分搭配固定（互信息）

信息熵：某条信息中所含的信息量

互信息：两个离散型随机变量X与Y相关程度的度量。互信息越大，两个随机变量的关联就越密切，同时发生的可能性就越大。
